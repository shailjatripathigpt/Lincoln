{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.602107375815354,
  "eval_steps": 500,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04014049172102358,
      "grad_norm": 1.8421502113342285,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 4.5713,
      "step": 10
    },
    {
      "epoch": 0.08028098344204716,
      "grad_norm": 0.8787457942962646,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 4.6657,
      "step": 20
    },
    {
      "epoch": 0.12042147516307075,
      "grad_norm": 0.7124553322792053,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 4.4638,
      "step": 30
    },
    {
      "epoch": 0.16056196688409433,
      "grad_norm": 0.5603380799293518,
      "learning_rate": 7.4e-05,
      "loss": 4.2933,
      "step": 40
    },
    {
      "epoch": 0.2007024586051179,
      "grad_norm": 0.5503683686256409,
      "learning_rate": 9.4e-05,
      "loss": 4.1585,
      "step": 50
    },
    {
      "epoch": 0.2408429503261415,
      "grad_norm": 0.7555639743804932,
      "learning_rate": 0.00011399999999999999,
      "loss": 4.1581,
      "step": 60
    },
    {
      "epoch": 0.2809834420471651,
      "grad_norm": 0.8049877882003784,
      "learning_rate": 0.000134,
      "loss": 3.9787,
      "step": 70
    },
    {
      "epoch": 0.32112393376818865,
      "grad_norm": 0.7909504175186157,
      "learning_rate": 0.000154,
      "loss": 4.0297,
      "step": 80
    },
    {
      "epoch": 0.3612644254892122,
      "grad_norm": 0.9371992945671082,
      "learning_rate": 0.000174,
      "loss": 3.9618,
      "step": 90
    },
    {
      "epoch": 0.4014049172102358,
      "grad_norm": 0.9441649317741394,
      "learning_rate": 0.000194,
      "loss": 3.8642,
      "step": 100
    },
    {
      "epoch": 0.44154540893125943,
      "grad_norm": 0.8336182832717896,
      "learning_rate": 0.000199,
      "loss": 3.9087,
      "step": 110
    },
    {
      "epoch": 0.481685900652283,
      "grad_norm": 1.012526512145996,
      "learning_rate": 0.00019757142857142857,
      "loss": 3.7185,
      "step": 120
    },
    {
      "epoch": 0.5218263923733065,
      "grad_norm": 0.8439316153526306,
      "learning_rate": 0.00019614285714285716,
      "loss": 3.8397,
      "step": 130
    },
    {
      "epoch": 0.5619668840943302,
      "grad_norm": 1.00399911403656,
      "learning_rate": 0.00019471428571428572,
      "loss": 3.7008,
      "step": 140
    },
    {
      "epoch": 0.6021073758153538,
      "grad_norm": 0.9682647585868835,
      "learning_rate": 0.00019328571428571428,
      "loss": 3.7491,
      "step": 150
    },
    {
      "epoch": 0.6422478675363773,
      "grad_norm": 1.1468530893325806,
      "learning_rate": 0.00019185714285714287,
      "loss": 3.8118,
      "step": 160
    },
    {
      "epoch": 0.6823883592574009,
      "grad_norm": 0.9590252637863159,
      "learning_rate": 0.00019042857142857145,
      "loss": 3.8077,
      "step": 170
    },
    {
      "epoch": 0.7225288509784245,
      "grad_norm": 0.8191327452659607,
      "learning_rate": 0.00018899999999999999,
      "loss": 3.6789,
      "step": 180
    },
    {
      "epoch": 0.7626693426994481,
      "grad_norm": 0.9909797310829163,
      "learning_rate": 0.00018757142857142857,
      "loss": 3.7034,
      "step": 190
    },
    {
      "epoch": 0.8028098344204716,
      "grad_norm": 1.1167703866958618,
      "learning_rate": 0.00018614285714285716,
      "loss": 3.6692,
      "step": 200
    },
    {
      "epoch": 0.8429503261414952,
      "grad_norm": 0.9590809345245361,
      "learning_rate": 0.00018471428571428572,
      "loss": 3.6917,
      "step": 210
    },
    {
      "epoch": 0.8830908178625189,
      "grad_norm": 0.7440033555030823,
      "learning_rate": 0.0001832857142857143,
      "loss": 3.7373,
      "step": 220
    },
    {
      "epoch": 0.9232313095835424,
      "grad_norm": 0.9488980770111084,
      "learning_rate": 0.00018185714285714287,
      "loss": 3.6413,
      "step": 230
    },
    {
      "epoch": 0.963371801304566,
      "grad_norm": 0.9717894196510315,
      "learning_rate": 0.00018042857142857143,
      "loss": 3.7186,
      "step": 240
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.750757098197937,
      "learning_rate": 0.00017900000000000001,
      "loss": 3.5981,
      "step": 250
    },
    {
      "epoch": 1.0401404917210235,
      "grad_norm": 1.1004048585891724,
      "learning_rate": 0.0001775714285714286,
      "loss": 3.5958,
      "step": 260
    },
    {
      "epoch": 1.0802809834420473,
      "grad_norm": 1.1043615341186523,
      "learning_rate": 0.00017614285714285713,
      "loss": 3.6598,
      "step": 270
    },
    {
      "epoch": 1.1204214751630708,
      "grad_norm": 1.0875651836395264,
      "learning_rate": 0.00017471428571428572,
      "loss": 3.6829,
      "step": 280
    },
    {
      "epoch": 1.1605619668840943,
      "grad_norm": 0.944510817527771,
      "learning_rate": 0.0001732857142857143,
      "loss": 3.6147,
      "step": 290
    },
    {
      "epoch": 1.2007024586051178,
      "grad_norm": 1.0040695667266846,
      "learning_rate": 0.00017185714285714287,
      "loss": 3.4705,
      "step": 300
    },
    {
      "epoch": 1.2408429503261416,
      "grad_norm": 1.3774847984313965,
      "learning_rate": 0.00017042857142857143,
      "loss": 3.6039,
      "step": 310
    },
    {
      "epoch": 1.280983442047165,
      "grad_norm": 1.33414888381958,
      "learning_rate": 0.00016900000000000002,
      "loss": 3.5945,
      "step": 320
    },
    {
      "epoch": 1.3211239337681886,
      "grad_norm": 0.951049268245697,
      "learning_rate": 0.00016757142857142857,
      "loss": 3.5626,
      "step": 330
    },
    {
      "epoch": 1.3612644254892121,
      "grad_norm": 0.9404031038284302,
      "learning_rate": 0.00016614285714285716,
      "loss": 3.4834,
      "step": 340
    },
    {
      "epoch": 1.4014049172102359,
      "grad_norm": 1.2459239959716797,
      "learning_rate": 0.00016471428571428572,
      "loss": 3.5931,
      "step": 350
    },
    {
      "epoch": 1.4415454089312594,
      "grad_norm": 1.2329891920089722,
      "learning_rate": 0.00016328571428571428,
      "loss": 3.527,
      "step": 360
    },
    {
      "epoch": 1.4816859006522831,
      "grad_norm": 0.9906928539276123,
      "learning_rate": 0.00016185714285714287,
      "loss": 3.6958,
      "step": 370
    },
    {
      "epoch": 1.5218263923733066,
      "grad_norm": 1.1057921648025513,
      "learning_rate": 0.00016042857142857143,
      "loss": 3.5326,
      "step": 380
    },
    {
      "epoch": 1.5619668840943302,
      "grad_norm": 1.0678304433822632,
      "learning_rate": 0.00015900000000000002,
      "loss": 3.6073,
      "step": 390
    },
    {
      "epoch": 1.6021073758153537,
      "grad_norm": 1.2674479484558105,
      "learning_rate": 0.00015757142857142858,
      "loss": 3.4348,
      "step": 400
    },
    {
      "epoch": 1.6422478675363772,
      "grad_norm": 1.3097560405731201,
      "learning_rate": 0.00015614285714285714,
      "loss": 3.5157,
      "step": 410
    },
    {
      "epoch": 1.682388359257401,
      "grad_norm": 1.3031721115112305,
      "learning_rate": 0.00015471428571428572,
      "loss": 3.4137,
      "step": 420
    },
    {
      "epoch": 1.7225288509784245,
      "grad_norm": 1.495369553565979,
      "learning_rate": 0.0001532857142857143,
      "loss": 3.5851,
      "step": 430
    },
    {
      "epoch": 1.7626693426994482,
      "grad_norm": 0.9848310947418213,
      "learning_rate": 0.00015185714285714284,
      "loss": 3.5124,
      "step": 440
    },
    {
      "epoch": 1.8028098344204717,
      "grad_norm": 1.0451788902282715,
      "learning_rate": 0.00015042857142857143,
      "loss": 3.533,
      "step": 450
    },
    {
      "epoch": 1.8429503261414952,
      "grad_norm": 1.1044682264328003,
      "learning_rate": 0.00014900000000000002,
      "loss": 3.6033,
      "step": 460
    },
    {
      "epoch": 1.8830908178625188,
      "grad_norm": 1.3584651947021484,
      "learning_rate": 0.00014757142857142858,
      "loss": 3.5682,
      "step": 470
    },
    {
      "epoch": 1.9232313095835423,
      "grad_norm": 1.0639640092849731,
      "learning_rate": 0.00014614285714285716,
      "loss": 3.53,
      "step": 480
    },
    {
      "epoch": 1.963371801304566,
      "grad_norm": 1.1797584295272827,
      "learning_rate": 0.00014471428571428572,
      "loss": 3.5527,
      "step": 490
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.9524105787277222,
      "learning_rate": 0.00014328571428571428,
      "loss": 3.5782,
      "step": 500
    },
    {
      "epoch": 2.0401404917210235,
      "grad_norm": 1.0705294609069824,
      "learning_rate": 0.00014185714285714287,
      "loss": 3.3937,
      "step": 510
    },
    {
      "epoch": 2.080280983442047,
      "grad_norm": 1.2159733772277832,
      "learning_rate": 0.00014042857142857143,
      "loss": 3.3853,
      "step": 520
    },
    {
      "epoch": 2.1204214751630706,
      "grad_norm": 1.2690668106079102,
      "learning_rate": 0.000139,
      "loss": 3.4803,
      "step": 530
    },
    {
      "epoch": 2.1605619668840945,
      "grad_norm": 0.9939882159233093,
      "learning_rate": 0.00013757142857142858,
      "loss": 3.389,
      "step": 540
    },
    {
      "epoch": 2.200702458605118,
      "grad_norm": 1.3063075542449951,
      "learning_rate": 0.00013614285714285714,
      "loss": 3.5252,
      "step": 550
    },
    {
      "epoch": 2.2408429503261416,
      "grad_norm": 1.0442743301391602,
      "learning_rate": 0.00013471428571428572,
      "loss": 3.487,
      "step": 560
    },
    {
      "epoch": 2.280983442047165,
      "grad_norm": 1.4725252389907837,
      "learning_rate": 0.0001332857142857143,
      "loss": 3.4673,
      "step": 570
    },
    {
      "epoch": 2.3211239337681886,
      "grad_norm": 1.3049473762512207,
      "learning_rate": 0.00013185714285714284,
      "loss": 3.4514,
      "step": 580
    },
    {
      "epoch": 2.361264425489212,
      "grad_norm": 1.4722295999526978,
      "learning_rate": 0.00013042857142857143,
      "loss": 3.4636,
      "step": 590
    },
    {
      "epoch": 2.4014049172102356,
      "grad_norm": 1.7866697311401367,
      "learning_rate": 0.00012900000000000002,
      "loss": 3.5168,
      "step": 600
    },
    {
      "epoch": 2.4415454089312596,
      "grad_norm": 1.1516786813735962,
      "learning_rate": 0.00012757142857142858,
      "loss": 3.4222,
      "step": 610
    },
    {
      "epoch": 2.481685900652283,
      "grad_norm": 1.2736817598342896,
      "learning_rate": 0.00012614285714285714,
      "loss": 3.4281,
      "step": 620
    },
    {
      "epoch": 2.5218263923733066,
      "grad_norm": 1.2186776399612427,
      "learning_rate": 0.00012471428571428573,
      "loss": 3.3792,
      "step": 630
    },
    {
      "epoch": 2.56196688409433,
      "grad_norm": 1.152705192565918,
      "learning_rate": 0.00012328571428571429,
      "loss": 3.4284,
      "step": 640
    },
    {
      "epoch": 2.6021073758153537,
      "grad_norm": 1.3541216850280762,
      "learning_rate": 0.00012185714285714287,
      "loss": 3.4986,
      "step": 650
    },
    {
      "epoch": 2.642247867536377,
      "grad_norm": 1.4632970094680786,
      "learning_rate": 0.00012042857142857142,
      "loss": 3.4602,
      "step": 660
    },
    {
      "epoch": 2.682388359257401,
      "grad_norm": 1.2498350143432617,
      "learning_rate": 0.000119,
      "loss": 3.3987,
      "step": 670
    },
    {
      "epoch": 2.7225288509784242,
      "grad_norm": 1.242351770401001,
      "learning_rate": 0.00011757142857142858,
      "loss": 3.3709,
      "step": 680
    },
    {
      "epoch": 2.762669342699448,
      "grad_norm": 1.4776108264923096,
      "learning_rate": 0.00011614285714285715,
      "loss": 3.5144,
      "step": 690
    },
    {
      "epoch": 2.8028098344204717,
      "grad_norm": 1.3563300371170044,
      "learning_rate": 0.00011471428571428573,
      "loss": 3.4569,
      "step": 700
    },
    {
      "epoch": 2.8429503261414952,
      "grad_norm": 1.004830002784729,
      "learning_rate": 0.00011328571428571429,
      "loss": 3.3779,
      "step": 710
    },
    {
      "epoch": 2.8830908178625188,
      "grad_norm": 1.586978554725647,
      "learning_rate": 0.00011185714285714286,
      "loss": 3.4564,
      "step": 720
    },
    {
      "epoch": 2.9232313095835423,
      "grad_norm": 1.6351971626281738,
      "learning_rate": 0.00011042857142857143,
      "loss": 3.394,
      "step": 730
    },
    {
      "epoch": 2.9633718013045662,
      "grad_norm": 1.0768537521362305,
      "learning_rate": 0.000109,
      "loss": 3.3804,
      "step": 740
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.4588356018066406,
      "learning_rate": 0.00010757142857142857,
      "loss": 3.3927,
      "step": 750
    },
    {
      "epoch": 3.0401404917210235,
      "grad_norm": 1.3442413806915283,
      "learning_rate": 0.00010614285714285714,
      "loss": 3.3302,
      "step": 760
    },
    {
      "epoch": 3.080280983442047,
      "grad_norm": 1.5365262031555176,
      "learning_rate": 0.00010471428571428571,
      "loss": 3.3654,
      "step": 770
    },
    {
      "epoch": 3.1204214751630706,
      "grad_norm": 1.6399540901184082,
      "learning_rate": 0.0001032857142857143,
      "loss": 3.3887,
      "step": 780
    },
    {
      "epoch": 3.1605619668840945,
      "grad_norm": 1.3639620542526245,
      "learning_rate": 0.00010185714285714285,
      "loss": 3.5013,
      "step": 790
    },
    {
      "epoch": 3.200702458605118,
      "grad_norm": 1.412113904953003,
      "learning_rate": 0.00010042857142857143,
      "loss": 3.3611,
      "step": 800
    },
    {
      "epoch": 3.2408429503261416,
      "grad_norm": 1.3691978454589844,
      "learning_rate": 9.900000000000001e-05,
      "loss": 3.3095,
      "step": 810
    },
    {
      "epoch": 3.280983442047165,
      "grad_norm": 1.5329358577728271,
      "learning_rate": 9.757142857142858e-05,
      "loss": 3.2214,
      "step": 820
    },
    {
      "epoch": 3.3211239337681886,
      "grad_norm": 1.7205584049224854,
      "learning_rate": 9.614285714285714e-05,
      "loss": 3.3672,
      "step": 830
    },
    {
      "epoch": 3.361264425489212,
      "grad_norm": 1.5878334045410156,
      "learning_rate": 9.471428571428573e-05,
      "loss": 3.3151,
      "step": 840
    },
    {
      "epoch": 3.4014049172102356,
      "grad_norm": 1.217307209968567,
      "learning_rate": 9.328571428571429e-05,
      "loss": 3.2826,
      "step": 850
    },
    {
      "epoch": 3.4415454089312596,
      "grad_norm": 1.6398266553878784,
      "learning_rate": 9.185714285714286e-05,
      "loss": 3.2773,
      "step": 860
    },
    {
      "epoch": 3.481685900652283,
      "grad_norm": 1.4032443761825562,
      "learning_rate": 9.042857142857143e-05,
      "loss": 3.4201,
      "step": 870
    },
    {
      "epoch": 3.5218263923733066,
      "grad_norm": 1.537184715270996,
      "learning_rate": 8.900000000000001e-05,
      "loss": 3.3962,
      "step": 880
    },
    {
      "epoch": 3.56196688409433,
      "grad_norm": 1.2745927572250366,
      "learning_rate": 8.757142857142857e-05,
      "loss": 3.3182,
      "step": 890
    },
    {
      "epoch": 3.6021073758153537,
      "grad_norm": 1.241547703742981,
      "learning_rate": 8.614285714285714e-05,
      "loss": 3.4069,
      "step": 900
    },
    {
      "epoch": 3.642247867536377,
      "grad_norm": 1.3908799886703491,
      "learning_rate": 8.471428571428573e-05,
      "loss": 3.2738,
      "step": 910
    },
    {
      "epoch": 3.682388359257401,
      "grad_norm": 1.39966881275177,
      "learning_rate": 8.328571428571429e-05,
      "loss": 3.345,
      "step": 920
    },
    {
      "epoch": 3.7225288509784242,
      "grad_norm": 1.3300533294677734,
      "learning_rate": 8.185714285714286e-05,
      "loss": 3.4092,
      "step": 930
    },
    {
      "epoch": 3.762669342699448,
      "grad_norm": 1.3427822589874268,
      "learning_rate": 8.042857142857144e-05,
      "loss": 3.3958,
      "step": 940
    },
    {
      "epoch": 3.8028098344204717,
      "grad_norm": 1.3854917287826538,
      "learning_rate": 7.900000000000001e-05,
      "loss": 3.2918,
      "step": 950
    },
    {
      "epoch": 3.8429503261414952,
      "grad_norm": 1.38289475440979,
      "learning_rate": 7.757142857142857e-05,
      "loss": 3.3954,
      "step": 960
    },
    {
      "epoch": 3.8830908178625188,
      "grad_norm": 1.2759150266647339,
      "learning_rate": 7.614285714285714e-05,
      "loss": 3.4224,
      "step": 970
    },
    {
      "epoch": 3.9232313095835423,
      "grad_norm": 1.232996940612793,
      "learning_rate": 7.471428571428572e-05,
      "loss": 3.4078,
      "step": 980
    },
    {
      "epoch": 3.9633718013045662,
      "grad_norm": 1.2674616575241089,
      "learning_rate": 7.328571428571429e-05,
      "loss": 3.3721,
      "step": 990
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.312675952911377,
      "learning_rate": 7.185714285714285e-05,
      "loss": 3.4123,
      "step": 1000
    },
    {
      "epoch": 4.040140491721024,
      "grad_norm": 1.2754243612289429,
      "learning_rate": 7.042857142857144e-05,
      "loss": 3.3554,
      "step": 1010
    },
    {
      "epoch": 4.080280983442047,
      "grad_norm": 1.5471453666687012,
      "learning_rate": 6.9e-05,
      "loss": 3.3424,
      "step": 1020
    },
    {
      "epoch": 4.120421475163071,
      "grad_norm": 1.7764047384262085,
      "learning_rate": 6.757142857142857e-05,
      "loss": 3.2597,
      "step": 1030
    },
    {
      "epoch": 4.160561966884094,
      "grad_norm": 1.5206576585769653,
      "learning_rate": 6.614285714285716e-05,
      "loss": 3.2832,
      "step": 1040
    },
    {
      "epoch": 4.200702458605118,
      "grad_norm": 1.377043604850769,
      "learning_rate": 6.471428571428572e-05,
      "loss": 3.3497,
      "step": 1050
    },
    {
      "epoch": 4.240842950326141,
      "grad_norm": 1.750164270401001,
      "learning_rate": 6.328571428571429e-05,
      "loss": 3.2511,
      "step": 1060
    },
    {
      "epoch": 4.280983442047165,
      "grad_norm": 1.4652873277664185,
      "learning_rate": 6.185714285714286e-05,
      "loss": 3.2521,
      "step": 1070
    },
    {
      "epoch": 4.321123933768189,
      "grad_norm": 1.4798526763916016,
      "learning_rate": 6.042857142857144e-05,
      "loss": 3.3622,
      "step": 1080
    },
    {
      "epoch": 4.361264425489212,
      "grad_norm": 1.30703604221344,
      "learning_rate": 5.9e-05,
      "loss": 3.2697,
      "step": 1090
    },
    {
      "epoch": 4.401404917210236,
      "grad_norm": 1.3252418041229248,
      "learning_rate": 5.757142857142858e-05,
      "loss": 3.3576,
      "step": 1100
    },
    {
      "epoch": 4.441545408931259,
      "grad_norm": 1.9544541835784912,
      "learning_rate": 5.6142857142857145e-05,
      "loss": 3.2668,
      "step": 1110
    },
    {
      "epoch": 4.481685900652283,
      "grad_norm": 1.4209280014038086,
      "learning_rate": 5.471428571428572e-05,
      "loss": 3.1667,
      "step": 1120
    },
    {
      "epoch": 4.521826392373306,
      "grad_norm": 1.3603606224060059,
      "learning_rate": 5.3285714285714285e-05,
      "loss": 3.2181,
      "step": 1130
    },
    {
      "epoch": 4.56196688409433,
      "grad_norm": 1.6771048307418823,
      "learning_rate": 5.185714285714286e-05,
      "loss": 3.336,
      "step": 1140
    },
    {
      "epoch": 4.602107375815354,
      "grad_norm": 1.8357752561569214,
      "learning_rate": 5.042857142857144e-05,
      "loss": 3.3831,
      "step": 1150
    },
    {
      "epoch": 4.642247867536377,
      "grad_norm": 1.5401511192321777,
      "learning_rate": 4.9e-05,
      "loss": 3.4259,
      "step": 1160
    },
    {
      "epoch": 4.682388359257401,
      "grad_norm": 1.3416550159454346,
      "learning_rate": 4.757142857142857e-05,
      "loss": 3.2852,
      "step": 1170
    },
    {
      "epoch": 4.722528850978424,
      "grad_norm": 1.2976367473602295,
      "learning_rate": 4.6142857142857145e-05,
      "loss": 3.3299,
      "step": 1180
    },
    {
      "epoch": 4.762669342699448,
      "grad_norm": 1.519607663154602,
      "learning_rate": 4.471428571428571e-05,
      "loss": 3.34,
      "step": 1190
    },
    {
      "epoch": 4.802809834420471,
      "grad_norm": 1.8058135509490967,
      "learning_rate": 4.328571428571429e-05,
      "loss": 3.3192,
      "step": 1200
    },
    {
      "epoch": 4.842950326141495,
      "grad_norm": 1.2172017097473145,
      "learning_rate": 4.185714285714286e-05,
      "loss": 3.3337,
      "step": 1210
    },
    {
      "epoch": 4.883090817862519,
      "grad_norm": 1.3450963497161865,
      "learning_rate": 4.042857142857143e-05,
      "loss": 3.2264,
      "step": 1220
    },
    {
      "epoch": 4.923231309583542,
      "grad_norm": 1.6381498575210571,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 3.2039,
      "step": 1230
    },
    {
      "epoch": 4.963371801304566,
      "grad_norm": 1.7232205867767334,
      "learning_rate": 3.757142857142857e-05,
      "loss": 3.3067,
      "step": 1240
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.194599628448486,
      "learning_rate": 3.6142857142857146e-05,
      "loss": 3.2654,
      "step": 1250
    },
    {
      "epoch": 5.040140491721024,
      "grad_norm": 1.5016919374465942,
      "learning_rate": 3.471428571428571e-05,
      "loss": 3.2454,
      "step": 1260
    },
    {
      "epoch": 5.080280983442047,
      "grad_norm": 1.613344669342041,
      "learning_rate": 3.3285714285714286e-05,
      "loss": 3.3143,
      "step": 1270
    },
    {
      "epoch": 5.120421475163071,
      "grad_norm": 1.6869227886199951,
      "learning_rate": 3.185714285714286e-05,
      "loss": 3.3142,
      "step": 1280
    },
    {
      "epoch": 5.160561966884094,
      "grad_norm": 1.771854043006897,
      "learning_rate": 3.042857142857143e-05,
      "loss": 3.2685,
      "step": 1290
    },
    {
      "epoch": 5.200702458605118,
      "grad_norm": 1.4520994424819946,
      "learning_rate": 2.9e-05,
      "loss": 3.2704,
      "step": 1300
    },
    {
      "epoch": 5.240842950326141,
      "grad_norm": 1.279075264930725,
      "learning_rate": 2.757142857142857e-05,
      "loss": 3.1962,
      "step": 1310
    },
    {
      "epoch": 5.280983442047165,
      "grad_norm": 1.2385468482971191,
      "learning_rate": 2.6142857142857147e-05,
      "loss": 3.1227,
      "step": 1320
    },
    {
      "epoch": 5.321123933768189,
      "grad_norm": 1.5297924280166626,
      "learning_rate": 2.4714285714285714e-05,
      "loss": 3.1917,
      "step": 1330
    },
    {
      "epoch": 5.361264425489212,
      "grad_norm": 1.385562539100647,
      "learning_rate": 2.3285714285714287e-05,
      "loss": 3.2517,
      "step": 1340
    },
    {
      "epoch": 5.401404917210236,
      "grad_norm": 1.5432912111282349,
      "learning_rate": 2.185714285714286e-05,
      "loss": 3.2923,
      "step": 1350
    },
    {
      "epoch": 5.441545408931259,
      "grad_norm": 1.5324360132217407,
      "learning_rate": 2.042857142857143e-05,
      "loss": 3.3213,
      "step": 1360
    },
    {
      "epoch": 5.481685900652283,
      "grad_norm": 1.430572748184204,
      "learning_rate": 1.9e-05,
      "loss": 3.2952,
      "step": 1370
    },
    {
      "epoch": 5.521826392373306,
      "grad_norm": 1.7646167278289795,
      "learning_rate": 1.757142857142857e-05,
      "loss": 3.3643,
      "step": 1380
    },
    {
      "epoch": 5.56196688409433,
      "grad_norm": 1.340822458267212,
      "learning_rate": 1.614285714285714e-05,
      "loss": 3.3101,
      "step": 1390
    },
    {
      "epoch": 5.602107375815354,
      "grad_norm": 1.5644822120666504,
      "learning_rate": 1.4714285714285713e-05,
      "loss": 3.2499,
      "step": 1400
    }
  ],
  "logging_steps": 10,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.063190206611456e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
